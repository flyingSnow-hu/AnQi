#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPUæ€§èƒ½ç›‘æ§å·¥å…·
æ˜¾ç¤ºGPUåˆ©ç”¨ç‡ã€æ˜¾å­˜ä½¿ç”¨ã€æ¸©åº¦ç­‰ä¿¡æ¯
"""
import sys
import os
import time

project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

try:
    import torch
    import pynvml
    has_nvml = True
    try:
        pynvml.nvmlInit()
    except:
        has_nvml = False
        print("è­¦å‘Š: æ— æ³•åˆå§‹åŒ–NVMLï¼Œéƒ¨åˆ†GPUä¿¡æ¯å¯èƒ½ä¸å¯ç”¨")
except ImportError:
    print("é”™è¯¯: éœ€è¦å®‰è£… nvidia-ml-py3")
    print("è¿è¡Œ: pip install nvidia-ml-py3")
    sys.exit(1)


def get_gpu_info():
    """è·å–GPUä¿¡æ¯"""
    if not torch.cuda.is_available():
        return None
    
    info = {
        'name': torch.cuda.get_device_name(0),
        'cuda_version': torch.version.cuda,
        'pytorch_version': torch.__version__,
        'device_count': torch.cuda.device_count()
    }
    
    # æ˜¾å­˜ä¿¡æ¯
    memory_allocated = torch.cuda.memory_allocated(0) / 1024**3  # GB
    memory_reserved = torch.cuda.memory_reserved(0) / 1024**3
    memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
    
    info['memory_allocated'] = memory_allocated
    info['memory_reserved'] = memory_reserved
    info['memory_total'] = memory_total
    info['memory_free'] = memory_total - memory_reserved
    
    # NVMLé¢å¤–ä¿¡æ¯
    if has_nvml:
        try:
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            
            # GPUåˆ©ç”¨ç‡
            util = pynvml.nvmlDeviceGetUtilizationRates(handle)
            info['gpu_util'] = util.gpu
            info['mem_util'] = util.memory
            
            # æ¸©åº¦
            temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
            info['temperature'] = temp
            
            # åŠŸç‡
            power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # W
            power_limit = pynvml.nvmlDeviceGetPowerManagementLimit(handle) / 1000.0
            info['power'] = power
            info['power_limit'] = power_limit
            
            # é£æ‰‡è½¬é€Ÿ
            try:
                fan_speed = pynvml.nvmlDeviceGetFanSpeed(handle)
                info['fan_speed'] = fan_speed
            except:
                info['fan_speed'] = None
                
        except Exception as e:
            print(f"è·å–NVMLä¿¡æ¯å¤±è´¥: {e}")
    
    return info


def print_gpu_info(info, clear_screen=True):
    """æ‰“å°GPUä¿¡æ¯"""
    if clear_screen:
        os.system('cls' if os.name == 'nt' else 'clear')
    
    print("="*70)
    print(" GPU æ€§èƒ½ç›‘æ§")
    print("="*70)
    print(f"GPUå‹å·: {info['name']}")
    print(f"CUDAç‰ˆæœ¬: {info['cuda_version']}")
    print(f"PyTorchç‰ˆæœ¬: {info['pytorch_version']}")
    print(f"GPUæ•°é‡: {info['device_count']}")
    print("-"*70)
    
    # æ˜¾å­˜ä½¿ç”¨
    mem_percent = (info['memory_reserved'] / info['memory_total']) * 100
    print(f"æ˜¾å­˜ä½¿ç”¨: {info['memory_reserved']:.2f} GB / {info['memory_total']:.2f} GB ({mem_percent:.1f}%)")
    print(f"  - å·²åˆ†é…: {info['memory_allocated']:.2f} GB")
    print(f"  - ç©ºé—²: {info['memory_free']:.2f} GB")
    
    # GPUåˆ©ç”¨ç‡
    if 'gpu_util' in info:
        print(f"\nGPUåˆ©ç”¨ç‡: {info['gpu_util']}%")
        print(f"æ˜¾å­˜æ§åˆ¶å™¨åˆ©ç”¨ç‡: {info['mem_util']}%")
    
    # æ¸©åº¦
    if 'temperature' in info:
        temp_bar = "â–ˆ" * int(info['temperature'] / 5) + "â–‘" * (20 - int(info['temperature'] / 5))
        print(f"\næ¸©åº¦: {info['temperature']}Â°C [{temp_bar}]")
    
    # åŠŸç‡
    if 'power' in info:
        power_percent = (info['power'] / info['power_limit']) * 100
        power_bar = "â–ˆ" * int(power_percent / 5) + "â–‘" * (20 - int(power_percent / 5))
        print(f"åŠŸç‡: {info['power']:.1f}W / {info['power_limit']:.1f}W ({power_percent:.1f}%)")
        print(f"      [{power_bar}]")
    
    # é£æ‰‡
    if info.get('fan_speed') is not None:
        fan_bar = "â–ˆ" * int(info['fan_speed'] / 5) + "â–‘" * (20 - int(info['fan_speed'] / 5))
        print(f"\né£æ‰‡è½¬é€Ÿ: {info['fan_speed']}%")
        print(f"         [{fan_bar}]")
    
    print("="*70)
    
    # ä¼˜åŒ–å»ºè®®
    if info.get('gpu_util', 100) < 70:
        print("\nâš  GPUåˆ©ç”¨ç‡è¾ƒä½ï¼Œå»ºè®®:")
        print("  - å¢åŠ  --parallel-games (å¹¶è¡Œæ¸¸æˆæ•°)")
        print("  - å¢åŠ  --batch-size (æ‰¹æ¬¡å¤§å°)")
        print("  - å¢åŠ  --num-channels æˆ– --num-res-blocks (ç½‘ç»œè§„æ¨¡)")
    
    if mem_percent < 50:
        print("\nğŸ’¡ æ˜¾å­˜åˆ©ç”¨ç‡è¾ƒä½ï¼Œå¯ä»¥:")
        print("  - å¢åŠ æ‰¹æ¬¡å¤§å°åˆ° 512 æˆ–æ›´é«˜")
        print("  - å¢åŠ ç½‘ç»œé€šé“æ•°åˆ° 512")
        print("  - å¢åŠ æ®‹å·®å—æ•°é‡åˆ° 30+")


def monitor_loop(interval=2):
    """æŒç»­ç›‘æ§GPU"""
    print("å¼€å§‹ç›‘æ§GPU (æŒ‰Ctrl+Cé€€å‡º)...\n")
    
    try:
        while True:
            info = get_gpu_info()
            if info:
                print_gpu_info(info, clear_screen=True)
            else:
                print("æœªæ£€æµ‹åˆ°GPU")
            
            time.sleep(interval)
            
    except KeyboardInterrupt:
        print("\n\nç›‘æ§å·²åœæ­¢")
        if has_nvml:
            pynvml.nvmlShutdown()


def show_quick_stats():
    """æ˜¾ç¤ºå¿«é€Ÿç»Ÿè®¡"""
    info = get_gpu_info()
    if not info:
        print("æœªæ£€æµ‹åˆ°GPU")
        return
    
    print_gpu_info(info, clear_screen=False)


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='GPUæ€§èƒ½ç›‘æ§')
    parser.add_argument('--interval', type=int, default=2, help='ç›‘æ§åˆ·æ–°é—´éš”(ç§’)')
    parser.add_argument('--once', action='store_true', help='åªæ˜¾ç¤ºä¸€æ¬¡ï¼Œä¸æŒç»­ç›‘æ§')
    
    args = parser.parse_args()
    
    if args.once:
        show_quick_stats()
    else:
        monitor_loop(args.interval)
